import subprocess
import shlex
import time
import os
import glob
import yaml
import math
from typing import Any, Dict, List, Optional

from json import JSONDecodeError

import requests
from menglong.agents.component.tool_manager import tool
from menglong.ml_model.schema import user
from menglong import Model
from menglong.utils.log import print_message, print_json, MessageType
import json


@tool
def terminal_command(command_str):
    """
    æ‰§è¡Œå­—ç¬¦ä¸²å½¢å¼çš„å‘½ä»¤è¡Œï¼Œå¹¶æ•è·è¾“å‡º

    å‚æ•°:
        command_str (str): å®Œæ•´çš„å‘½ä»¤è¡Œå­—ç¬¦ä¸²

    è¿”å›:
        dict: åŒ…å«å‘½ä»¤æ‰§è¡Œç»“æœçš„å­—å…¸ï¼ŒåŒ…æ‹¬é€€å‡ºç ã€è¾“å‡ºå†…å®¹ç­‰
    """
    try:
        # ä½¿ç”¨shlexåˆ†å‰²å‘½ä»¤å­—ç¬¦ä¸²ï¼ˆå¤„ç†å¼•å·/ç©ºæ ¼ç­‰ç‰¹æ®Šå­—ç¬¦ï¼‰
        command_list = shlex.split(command_str)

        # å¯åŠ¨å­è¿›ç¨‹å¹¶æ•è·è¾“å‡º
        process = subprocess.Popen(
            command_list,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,  # åˆ†åˆ«æ•è·æ ‡å‡†è¾“å‡ºå’Œé”™è¯¯è¾“å‡º
            text=True,  # è¿”å›å­—ç¬¦ä¸²è€Œéå­—èŠ‚
            universal_newlines=True,
        )

        # è·å–è¾“å‡º
        stdout, stderr = process.communicate()

        # æ‰“å°è¾“å‡ºä¾›å®æ—¶æŸ¥çœ‹
        if stdout:
            print("STDOUT:")
            print(stdout)
        if stderr:
            print("STDERR:")
            print(stderr)

        # result = {
        #     "command": command_str,
        #     "exit_code": process.returncode,
        #     "stdout": stdout,
        #     "stderr": stderr,
        #     "success": process.returncode == 0,
        #     "output": stdout if stdout else stderr,  # ä¸»è¦è¾“å‡ºå†…å®¹
        # }
        result = stdout if stdout else stderr

        return result

    except FileNotFoundError:
        error_msg = f"é”™è¯¯ï¼šæ‰¾ä¸åˆ°å‘½ä»¤æˆ–å¯æ‰§è¡Œæ–‡ä»¶ '{command_list[0]}'"
        print(error_msg)
        return {
            "command": command_str,
            "exit_code": 127,
            "stdout": "",
            "stderr": error_msg,
            "success": False,
            "output": error_msg,
        }
    except Exception as e:
        error_msg = f"æ‰§è¡Œå‘½ä»¤æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
        print(error_msg)
        return {
            "command": command_str,
            "exit_code": 1,
            "stdout": "",
            "stderr": error_msg,
            "success": False,
            "output": error_msg,
        }


@tool
def plan_task(description: str, tools: Dict) -> Dict:
    """LLMç”Ÿæˆè¯¦ç»†æ‰§è¡Œè®¡åˆ’"""
    model = Model()
    available_tools = [name for name, tool in tools.items()]
    for name, tool in tools.items():
        if not tool.get("enabled", True):
            continue
        if tool.get("type") == "function":
            model.register_tool(
                name=name,
                function=tool["function"],
                description=tool.get("description", ""),
                **tool.get("kwargs", {}),
            )
        elif tool.get("type") == "command":
            model.register_tool(
                name=name,
                command=tool["command"],
                description=tool.get("description", ""),
            )
    prompt = f"""
ä¸ºä»»åŠ¡åˆ¶å®šè¯¦ç»†çš„æ‰§è¡Œè®¡åˆ’ï¼š

ä»»åŠ¡ï¼š{description}

å½“å‰å¯ç”¨çš„å·¥å…·: {available_tools}

è¦æ±‚ï¼š
å¦‚æœæ˜¯ç®€å•ä»»åŠ¡(simple)
    - ç›´æ¥ç›´æ¥æ‰§è¡Œï¼Œè·å¾—ç»“æœï¼Œæ— éœ€è§„åˆ’ã€‚
å¦‚æœæ˜¯å¤æ‚ä»»åŠ¡(complex)
    1. åˆ†è§£ä¸ºå…·ä½“çš„å­ä»»åŠ¡
    2. æ˜ç¡®æ¯ä¸ªå­ä»»åŠ¡çš„ä¾èµ–å…³ç³»
    3. æŒ‡å®šæ‰§è¡Œå·¥å…·
    4. é¢„æœŸè¾“å‡ºå’ŒæˆåŠŸæ ‡å‡†è¦ä¸ä»»åŠ¡ç›®æ ‡å’Œç›¸å…³ä¾èµ–ä»»åŠ¡ä¿æŒä¸€è‡´

çˆ¶ä»»åŠ¡å­ä»»åŠ¡çš„åˆ’åˆ†æŒ‰ç…§ä»¥ä¸‹è§„åˆ™:
    1. æ ¹æ®æ¨¡å—çš„åŠŸèƒ½åˆ’åˆ†ï¼Œä¸ºäº†æ˜ç¡®æ¯ä¸ªå­ä»»åŠ¡çš„èŒè´£
    2. æ ¹æ®ä»»åŠ¡çš„è¾“å…¥è¾“å‡ºå…³ç³»åˆ’åˆ†,ç¡®ä¿æ— ç¼ºæ— æ¼å®Œæˆçˆ¶ä»»åŠ¡
    3. ä¸ä»»åŠ¡çš„ä¾èµ–å…³ç³»æ­£äº¤,ä¸ä»¥ä¾èµ–å…³ç³»ä¸ºä¸»,ä»¥åŠŸèƒ½åˆ’åˆ†ä¸ºä¸»

è¿”å›JSONæ ¼å¼:
{{
  "task_tag": "ä»»åŠ¡tag",
  "task_type": "complex/simple",  # ä»»åŠ¡ç±»å‹
  "description": "ä»»åŠ¡çš„ç›®çš„æè¿°",
  "subtasks": [
    {{
      "task_tag": "ä»»åŠ¡tag",
      "task_type": "simple/complex",
      "description": "ä»»åŠ¡çš„è¯¦ç»†æè¿°",
      "dependencies": [],  # ä¾èµ–çš„task_tagåˆ—è¡¨
      "parent": "çˆ¶ä»»åŠ¡çš„task_tag",
      "tool_require": [],  # éœ€è¦çš„å·¥å…·åˆ—è¡¨
      "subtasks": [],  # å­ä»»åŠ¡åˆ—è¡¨
      "expected_output": "é¢„æœŸçš„è¾“å‡ºç»“æœæè¿°",
      "success_criteria": "éªŒè¯ä»»åŠ¡å®Œæˆçš„æ ‡å‡†",
    }},
  ],
  "success_criteria": "ä»»åŠ¡æœ€ç»ˆç›®æ ‡ï¼Œé‡åŒ–çš„ç»“æœ,æ•´ä½“ä»»åŠ¡å®Œæˆæ ‡å‡†"
}}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""
    try:
        start_time = time.time()
        response = model.chat([user(content=prompt)])
        response_text = response.message.content.text
        print_message(
            f"{response_text[:500]}", title="Raw response text"
        )  # è°ƒè¯•è¾“å‡ºå‰500ä¸ªå­—ç¬¦

        # è§£æJSON
        if response_text.startswith("```json"):
            response_text = response_text[7:]
        if response_text.endswith("```"):
            response_text = response_text[:-3]
        execution_plan = json.loads(response_text)
        print_json(
            execution_plan,
            title="Parsed execution plan subtasks",
        )
        end_time = time.time()
        print_message(f"Execution time: {end_time - start_time:.2f} seconds")
        return execution_plan

    except JSONDecodeError as e:
        print_message(f"JSON decode error: {e}")
        raise ValueError(f"Failed to parse execution plan JSON: {e}")
    except Exception as e:
        raise ValueError(f"Failed to generate execution plan: {e}") from e


@tool
def web_search(query: str, max_results: int = 3) -> Dict[str, Any]:
    """ä½¿ç”¨DuckDuckGoè¿›è¡ŒçœŸå®çš„ç½‘ç»œæœç´¢"""
    try:
        from duckduckgo_search import DDGS

        print_message(f"ğŸ” æ­£åœ¨æœç´¢: {query}")

        results = []
        with DDGS() as ddgs:
            # æ‰§è¡Œæœç´¢ï¼Œé™åˆ¶ç»“æœæ•°é‡
            search_results = list(ddgs.text(query, max_results=max_results))

            print_json(search_results, title="Raw search results")
            for i, result in enumerate(search_results):
                results.append(
                    {
                        "title": result.get("title", ""),
                        "url": result.get("href", ""),
                        "snippet": result.get("body", ""),
                        "relevance": 1.0 - (i * 0.1),  # æŒ‰é¡ºåºé€’å‡ç›¸å…³æ€§
                    }
                )

        print_message(f"âœ… æœç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(results)} ä¸ªç»“æœ", MessageType.SUCCESS)

        return {
            "query": query,
            "results": results,
            "total_found": len(results),
            "search_time": time.time(),
            "source": "DuckDuckGo",
        }

    except Exception as e:
        print_message(f"âŒ æœç´¢å‡ºé”™: {str(e)}", MessageType.ERROR)
        return {
            "query": query,
            "results": [],
            "total_found": 0,
            "error": str(e),
            "source": "DuckDuckGo",
        }


@tool
def brave_web_search(query: str, max_results: int = 1) -> Dict[str, Any]:
    """
    ä½¿ç”¨Brave Search APIè¿›è¡ŒçœŸå®çš„ç½‘ç»œæœç´¢

    Args:
        query (str): æœç´¢æŒ‰é’®
        max_results (int, optional): æœ€å¤§è¿”å›ç»“æœæ•°ï¼Œ Defaults to 1.

    Returns:
        Dict[str, Any]: æœç´¢å›¾åƒåˆ†ç±»
    """
    # Brave Search API URL
    url = "https://api.search.brave.com/res/v1/web/search"

    # è®¾ç½®è¯·æ±‚å¤´
    headers = {
        "Accept": "application/json",
        "Accept-Encoding": "gzip",
        "X-Subscription-Token": "<YOUR_API_KEY>",
    }

    # è®¾ç½®è¯·æ±‚å‚æ•°
    params = {"q": query, "limit": max_results}

    # try:
    # å‘é€è¯·æ±‚
    response = requests.get(url, headers=headers, params=params)
    print_message(f"ğŸ” æ­£åœ¨æœç´¢: {query}")
    # æ£€æŸ¥å“åº”çŠ¶æ€ç 
    if response.status_code == 200:
        # è§£æJSONå“åº”
        result = response.json()
        print_json(result, title="Raw search results")

        # æå–æœç´¢ç»“æœ
        if "web" in result and "results" in result["web"]:
            results = result["web"]["results"]
            return {"status": "success", "results": results[:max_results]}
        else:
            return {"status": "error", "message": "No results found"}
    else:
        return {
            "status": "error",
            "message": f"API request failed with status code {response.status_code}",
        }

    # except requests.exceptions.RequestException as e:
    #     return {"status": "error", "message": f"Request error: {str(e)}"}


@tool
def recall_experience(query: str) -> str:
    """
    ä½¿ç”¨RAGæ–¹å¼å›å¿†ç›¸å…³ç»éªŒ

    å‚æ•°:
        query (str): æ€è€ƒçš„å†…å®¹æˆ–ç»éªŒæŸ¥è¯¢

    è¿”å›:
        str: æ£€ç´¢åˆ°çš„ç›¸å…³ç»éªŒå†…å®¹
    """
    print_message(f"ğŸ’­ æ­£åœ¨æ£€ç´¢ç›¸å…³ç»éªŒ: {query}")

    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = Model()

        # ç»éªŒåº“æ–‡ä»¶è·¯å¾„
        experience_file = (
            "/Users/own/Workspace/MengLong/examples/app/terminal_try/experience.yaml"
        )

        # æ£€æŸ¥ç»éªŒåº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(experience_file):
            print_message("ç»éªŒåº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºç»“æœ")
            return "æš‚æ— ç›¸å…³ç»éªŒè®°å½•ã€‚"

        # è¯»å–ç»éªŒåº“
        try:
            with open(experience_file, "r", encoding="utf-8") as f:
                experience_data = yaml.safe_load(f) or {}
        except Exception as e:
            print_message(f"è¯»å–ç»éªŒåº“å¤±è´¥: {e}")
            return "æ— æ³•è¯»å–ç»éªŒåº“ã€‚"

        experiences = experience_data.get("experiences", [])
        if not experiences:
            return "ç»éªŒåº“ä¸ºç©ºï¼Œæš‚æ— ç›¸å…³ç»éªŒè®°å½•ã€‚"

        # ä½¿ç”¨å‘é‡æœç´¢æ‰¾åˆ°æœ€ç›¸å…³çš„ç»éªŒ
        top_experiences = _vector_search(query, experiences, model)

        if not top_experiences:
            # é™çº§ä¸ºå…³é”®è¯æœç´¢
            top_experiences = _keyword_search_experiences(query, experiences)

        if not top_experiences:
            return "æœªæ‰¾åˆ°ç›¸å…³çš„ç»éªŒè®°å½•ã€‚æ‚¨å¯èƒ½éœ€è¦ç§¯ç´¯æ›´å¤šç›¸å…³ç»éªŒã€‚"

        # ä½¿ç”¨LLMæ•´åˆå’Œæ€»ç»“æ£€ç´¢åˆ°çš„ç»éªŒ
        context_prompt = f"""
åŸºäºä»¥ä¸‹æŸ¥è¯¢å’Œæ£€ç´¢åˆ°çš„ç›¸å…³ç»éªŒï¼Œæä¾›æœ‰ç”¨çš„å»ºè®®å’ŒæŒ‡å¯¼ï¼š

æŸ¥è¯¢ï¼š{query}

ç›¸å…³ç»éªŒè®°å½•ï¼š
"""

        for i, item in enumerate(top_experiences):
            exp = item["experience"]
            similarity = item.get("similarity", 0)
            context_prompt += f"""
ç»éªŒ {i+1} (ç›¸å…³åº¦: {similarity:.2f}):
æ ‡é¢˜: {exp.get('title', 'æœªçŸ¥')}
åˆ†ç±»: {exp.get('category', 'æœªçŸ¥')}
æ‘˜è¦: {exp.get('summary', 'æ— æ‘˜è¦')}
å…³é”®ç‚¹: {', '.join(exp.get('details', {}).get('key_points', []))}
é€‚ç”¨åœºæ™¯: {', '.join(exp.get('details', {}).get('applicable_scenarios', []))}
æ³¨æ„äº‹é¡¹: {', '.join(exp.get('details', {}).get('pitfalls', []))}

"""

        context_prompt += """
è¯·åŸºäºè¿™äº›ç»éªŒè®°å½•ï¼Œä¸ºç”¨æˆ·çš„æŸ¥è¯¢æä¾›ï¼š
1. ç›´æ¥ç›¸å…³çš„è§£å†³æ–¹æ¡ˆæˆ–å»ºè®®
2. éœ€è¦æ³¨æ„çš„å…³é”®ç‚¹å’Œé™·é˜±
3. é€‚ç”¨çš„æœ€ä½³å®è·µ
4. å¦‚æœç»éªŒä¸å¤Ÿå®Œå…¨åŒ¹é…ï¼Œè¯·è¯´æ˜å“ªäº›éƒ¨åˆ†å¯ä»¥å‚è€ƒ

è¿”å›ç»“æ„åŒ–çš„å»ºè®®ï¼Œè¦å®ç”¨å’Œå…·ä½“ã€‚
"""

        start_time = time.time()
        response = model.chat([user(content=context_prompt)])
        final_advice = response.message.content.text

        end_time = time.time()
        print_message(
            f"âœ… ç»éªŒæ£€ç´¢å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’",
            MessageType.SUCCESS,
        )
        print_message(
            f"æ‰¾åˆ° {len(top_experiences)} æ¡ç›¸å…³ç»éªŒ", title="Retrieved Experience"
        )

        return final_advice

    except Exception as e:
        error_msg = f"ç»éªŒæ£€ç´¢å¤±è´¥: {str(e)}"
        print_message(f"âŒ {error_msg}", MessageType.ERROR)
        return f"ç»éªŒæ£€ç´¢é‡åˆ°é—®é¢˜: {error_msg}"


def _vector_search(query: str, experiences: List[Dict], model: Model) -> List[Dict]:
    """ä½¿ç”¨ menglong æ¨¡å‹è¿›è¡Œå‘é‡ç›¸ä¼¼åº¦æœç´¢"""
    try:
        # ç”ŸæˆæŸ¥è¯¢å‘é‡
        query_embedding = model.embed([query])
        if not query_embedding or len(query_embedding.embeddings) == 0:
            print_message("æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥ï¼Œé™çº§ä¸ºå…³é”®è¯æœç´¢")
            return []

        query_vector = query_embedding.embeddings
        relevant_items = []

        # è®¡ç®—ä¸æ¯ä¸ªç»éªŒçš„ç›¸ä¼¼åº¦
        for exp in experiences:
            if "embedding" not in exp:
                continue  # è·³è¿‡æ²¡æœ‰å‘é‡çš„ç»éªŒ

            exp_vector = exp["embedding"]
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarity = _cosine_similarity(query_vector, exp_vector)

            # ç›¸ä¼¼åº¦é˜ˆå€¼ä¸º 0.3
            if similarity > 0.3:
                relevant_items.append(
                    {"similarity": float(similarity), "experience": exp}
                )

        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰3ä¸ª
        relevant_items.sort(key=lambda x: x["similarity"], reverse=True)
        print_message(f"å‘é‡æœç´¢æ‰¾åˆ° {len(relevant_items)} æ¡ç›¸å…³ç»éªŒ")
        return relevant_items[:3]

    except Exception as e:
        print_message(f"å‘é‡æœç´¢å¤±è´¥: {e}")
        return []


def _keyword_search_experiences(query: str, experiences: List[Dict]) -> List[Dict]:
    """å…³é”®è¯åŒ¹é…æœç´¢"""
    query_keywords = query.lower().split()
    matches = []

    for exp in experiences:
        # æ„å»ºæœç´¢æ–‡æœ¬
        searchable_text = f"{exp.get('title', '')} {exp.get('summary', '')} {' '.join(exp.get('tags', []))}".lower()

        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        match_count = sum(1 for keyword in query_keywords if keyword in searchable_text)
        if match_count > 0:
            matches.append(
                {"similarity": match_count / len(query_keywords), "experience": exp}
            )

    # æŒ‰åŒ¹é…åº¦æ’åºï¼Œå–å‰3ä¸ª
    matches.sort(key=lambda x: x["similarity"], reverse=True)
    print_message(f"å…³é”®è¯æœç´¢æ‰¾åˆ° {len(matches)} æ¡ç›¸å…³ç»éªŒ")
    return matches[:3]


def _cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    """è®¡ç®—ä¸¤ä¸ªå‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦"""
    try:
        # è®¡ç®—ç‚¹ç§¯
        dot_product = sum(a * b for a, b in zip(vec1, vec2))

        # è®¡ç®—å‘é‡é•¿åº¦
        norm1 = math.sqrt(sum(a * a for a in vec1))
        norm2 = math.sqrt(sum(a * a for a in vec2))

        # é¿å…é™¤é›¶
        if norm1 == 0 or norm2 == 0:
            return 0.0

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
        return dot_product / (norm1 * norm2)
    except Exception:
        return 0.0


@tool
def summarize_and_save_experience(experience: str) -> str:
    """
    æ€»ç»“å¹¶ä¿å­˜å®è·µç»éªŒåˆ° YAML æ–‡ä»¶

    å‚æ•°:
        experience (str): è¦ä¿å­˜çš„å®è·µç»éªŒå†…å®¹

    è¿”å›:
        str: ä¿å­˜ç»“æœçš„ç¡®è®¤ä¿¡æ¯
    """
    print_message(f"ğŸ’¾ æ­£åœ¨æ€»ç»“å¹¶ä¿å­˜å®è·µç»éªŒ...")

    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = Model()

        # æ„å»ºæ€»ç»“æç¤ºè¯
        summary_prompt = f"""
è¯·å¯¹ä»¥ä¸‹å®è·µç»éªŒè¿›è¡Œç»“æ„åŒ–æ€»ç»“å’Œæç‚¼ï¼š

åŸå§‹ç»éªŒå†…å®¹ï¼š
{experience}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼æ€»ç»“ï¼š
{{
  "title": "ç»éªŒæ ‡é¢˜ï¼ˆç®€æ´æ˜äº†ï¼‰",
  "category": "ç»éªŒåˆ†ç±»ï¼ˆå¦‚ï¼šå¼€å‘æŠ€å·§ã€é—®é¢˜è§£å†³ã€æœ€ä½³å®è·µç­‰ï¼‰",
  "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2", "æ ‡ç­¾3"],
  "summary": "æ ¸å¿ƒè¦ç‚¹æ€»ç»“",
  "details": {{
    "problem": "é‡åˆ°çš„é—®é¢˜æˆ–åœºæ™¯",
    "solution": "è§£å†³æ–¹æ¡ˆæˆ–æ–¹æ³•",
    "key_points": ["å…³é”®è¦ç‚¹1", "å…³é”®è¦ç‚¹2"],
    "pitfalls": ["æ³¨æ„äº‹é¡¹æˆ–é™·é˜±"],
    "applicable_scenarios": ["é€‚ç”¨åœºæ™¯1", "é€‚ç”¨åœºæ™¯2"]
  }},
  "difficulty": "éš¾åº¦ç­‰çº§ï¼ˆç®€å•/ä¸­ç­‰/å›°éš¾ï¼‰",
  "usefulness": "å®ç”¨æ€§è¯„åˆ†ï¼ˆ1-5åˆ†ï¼‰"
}}

åªè¿”å›JSONæ ¼å¼ï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""

        start_time = time.time()
        response = model.chat([user(content=summary_prompt)])
        summary_text = response.message.content.text

        # è§£æJSON
        if summary_text.startswith("```json"):
            summary_text = summary_text[7:]
        if summary_text.endswith("```"):
            summary_text = summary_text[:-3]

        try:
            structured_experience = json.loads(summary_text)
            print_json(structured_experience, title="Structured Experience Summary")

            # ç»éªŒåº“æ–‡ä»¶è·¯å¾„
            experience_file = "/Users/own/Workspace/MengLong/examples/app/terminal_try/experience.yaml"

            # è¯»å–ç°æœ‰ç»éªŒåº“
            experience_data = {"experiences": []}
            if os.path.exists(experience_file):
                try:
                    with open(experience_file, "r", encoding="utf-8") as f:
                        experience_data = yaml.safe_load(f) or {"experiences": []}
                except Exception as e:
                    print_message(f"è¯»å–ç°æœ‰ç»éªŒåº“å¤±è´¥: {e}")

            # æ·»åŠ å…ƒæ•°æ®
            experience_id = f"exp_{int(time.time())}"
            structured_experience["id"] = experience_id
            structured_experience["created_at"] = time.time()
            structured_experience["raw_content"] = experience

            # ç”Ÿæˆæœç´¢æ–‡æœ¬ç”¨äºåç»­æ£€ç´¢
            searchable_text = f"{structured_experience.get('title', '')} {structured_experience.get('summary', '')} {' '.join(structured_experience.get('tags', []))}"

            # ä½¿ç”¨ menglong çš„ embed å‡½æ•°ç”Ÿæˆå‘é‡
            try:
                embedding = model.embed([searchable_text])
                if embedding and len(embedding.embeddings) > 0:
                    structured_experience["embedding"] = embedding.embeddings
                    print_message("âœ… ç»éªŒå‘é‡åŒ–æˆåŠŸ")
                else:
                    print_message("âš ï¸ å‘é‡åŒ–å¤±è´¥ï¼Œå°†ä¸æ”¯æŒè¯­ä¹‰æœç´¢")
            except Exception as e:
                print_message(f"âš ï¸ å‘é‡åŒ–å¤±è´¥: {e}")

            # æ·»åŠ åˆ°ç»éªŒåº“
            if "experiences" not in experience_data:
                experience_data["experiences"] = []
            experience_data["experiences"].append(structured_experience)

            # ä¿å­˜åˆ° YAML æ–‡ä»¶
            with open(experience_file, "w", encoding="utf-8") as f:
                yaml.dump(
                    experience_data,
                    f,
                    default_flow_style=False,
                    allow_unicode=True,
                    indent=2,
                )

            end_time = time.time()
            print_message(
                f"âœ… ç»éªŒæ€»ç»“å’Œä¿å­˜å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’",
                MessageType.SUCCESS,
            )
            print_message(f"ä¿å­˜ä½ç½®: {experience_file}", title="Experience Saved")

            return f"å®è·µç»éªŒå·²æˆåŠŸæ€»ç»“å¹¶ä¿å­˜ã€‚ç»éªŒID: {experience_id}ï¼Œæ ‡é¢˜: {structured_experience.get('title', 'æœªçŸ¥')}"

        except JSONDecodeError as e:
            print_message(f"JSONè§£æé”™è¯¯: {e}")
            # é™çº§å¤„ç†ï¼šç›´æ¥ä¿å­˜åŸå§‹å†…å®¹
            fallback_experience = {
                "id": f"raw_{int(time.time())}",
                "title": "åŸå§‹ç»éªŒè®°å½•",
                "category": "æœªåˆ†ç±»",
                "tags": ["åŸå§‹è®°å½•"],
                "summary": (
                    experience[:200] + "..." if len(experience) > 200 else experience
                ),
                "raw_content": experience,
                "created_at": time.time(),
                "details": {
                    "problem": "å¾…æ•´ç†",
                    "solution": "å¾…æ•´ç†",
                    "key_points": ["åŸå§‹è®°å½•å¾…æ•´ç†"],
                    "pitfalls": [],
                    "applicable_scenarios": [],
                },
                "difficulty": "æœªçŸ¥",
                "usefulness": "æœªè¯„åˆ†",
            }

            # è¯»å–ç°æœ‰ç»éªŒåº“å¹¶æ·»åŠ 
            experience_data = {"experiences": []}
            if os.path.exists(experience_file):
                try:
                    with open(experience_file, "r", encoding="utf-8") as f:
                        experience_data = yaml.safe_load(f) or {"experiences": []}
                except:
                    pass

            experience_data["experiences"].append(fallback_experience)

            with open(experience_file, "w", encoding="utf-8") as f:
                yaml.dump(
                    experience_data,
                    f,
                    default_flow_style=False,
                    allow_unicode=True,
                    indent=2,
                )

            return f"ç»éªŒä¿å­˜æˆåŠŸï¼ˆåŸå§‹æ ¼å¼ï¼‰ï¼ŒID: {fallback_experience['id']}"

    except Exception as e:
        error_msg = f"ç»éªŒæ€»ç»“å’Œä¿å­˜å¤±è´¥: {str(e)}"
        print_message(f"âŒ {error_msg}", MessageType.ERROR)
        return f"ç»éªŒä¿å­˜é‡åˆ°é—®é¢˜: {error_msg}"


@tool
def abstract_experience(domain: str = "", pattern_type: str = "all") -> str:
    """
    ä»ç»éªŒåº“ä¸­æŠ½è±¡åŒ–ç»éªŒæ¨¡å¼å’Œé€šç”¨è§„å¾‹

    å‚æ•°:
        domain (str): è¦æŠ½è±¡çš„é¢†åŸŸæˆ–ä¸»é¢˜ï¼ˆå¦‚ï¼š"ç¼–ç¨‹è°ƒè¯•", "é¡¹ç›®ç®¡ç†", "é—®é¢˜è§£å†³"ç­‰ï¼‰
        pattern_type (str): æŠ½è±¡æ¨¡å¼ç±»å‹ ("all", "success_patterns", "failure_patterns", "best_practices")

    è¿”å›:
        str: æŠ½è±¡åŒ–çš„ç»éªŒæ¨¡å¼å’Œè§„å¾‹æ€»ç»“
    """
    if domain == "":
        domain = "å…¨éƒ¨"
    print_message(f"ğŸ§  æ­£åœ¨æŠ½è±¡ '{domain}' é¢†åŸŸçš„ç»éªŒæ¨¡å¼ï¼Œç±»å‹: {pattern_type}")

    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = Model()

        # ç»éªŒåº“æ–‡ä»¶è·¯å¾„
        experience_file = (
            "/Users/own/Workspace/MengLong/examples/app/terminal_try/experience.yaml"
        )

        # æ£€æŸ¥ç»éªŒåº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(experience_file):
            return "ç»éªŒåº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œç»éªŒæŠ½è±¡ã€‚"

        # è¯»å–ç»éªŒåº“
        try:
            with open(experience_file, "r", encoding="utf-8") as f:
                experience_data = yaml.safe_load(f) or {}
        except Exception as e:
            print_message(f"è¯»å–ç»éªŒåº“å¤±è´¥: {e}")
            return "æ— æ³•è¯»å–ç»éªŒåº“æ–‡ä»¶ã€‚"

        experiences = experience_data.get("experiences", [])
        if not experiences:
            return "ç»éªŒåº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œç»éªŒæŠ½è±¡ã€‚"

        if domain == "å…¨éƒ¨":
            print_message("æŠ½è±¡å…¨éƒ¨é¢†åŸŸçš„ç»éªŒ")
            # å¦‚æœæ˜¯å…¨éƒ¨é¢†åŸŸï¼Œåˆ™ä¸è¿›è¡Œé¢†åŸŸç­›é€‰
            relevant_experiences = experiences
        else:
            print_message(f"æŠ½è±¡é¢†åŸŸ: {domain}")
            # æ ¹æ®é¢†åŸŸç­›é€‰ç›¸å…³ç»éªŒ
            relevant_experiences = _filter_experiences_by_domain(
                domain, experiences, model
            )

        if not relevant_experiences:
            return f"åœ¨ '{domain}' é¢†åŸŸæœªæ‰¾åˆ°ç›¸å…³ç»éªŒï¼Œæ— æ³•è¿›è¡ŒæŠ½è±¡ã€‚"

        print_message(f"æ‰¾åˆ° {len(relevant_experiences)} æ¡ç›¸å…³ç»éªŒè¿›è¡ŒæŠ½è±¡")

        # æ„å»ºæŠ½è±¡åŒ–æç¤ºè¯
        abstraction_prompt = _build_abstraction_prompt(
            domain, pattern_type, relevant_experiences
        )

        # ä½¿ç”¨LLMè¿›è¡Œç»éªŒæŠ½è±¡
        start_time = time.time()
        response = model.chat([user(content=abstraction_prompt)])
        abstracted_patterns = response.message.content.text

        end_time = time.time()
        print_message(
            f"âœ… ç»éªŒæŠ½è±¡å®Œæˆï¼Œè€—æ—¶: {end_time - start_time:.2f} ç§’",
            MessageType.SUCCESS,
        )

        # å¯é€‰ï¼šå°†æŠ½è±¡ç»“æœä¿å­˜ä¸ºæ–°çš„é«˜çº§ç»éªŒ
        _save_abstract_pattern(
            domain, pattern_type, abstracted_patterns, len(relevant_experiences)
        )

        return abstracted_patterns

    except Exception as e:
        error_msg = f"ç»éªŒæŠ½è±¡å¤±è´¥: {str(e)}"
        print_message(f"âŒ {error_msg}", MessageType.ERROR)
        return f"ç»éªŒæŠ½è±¡é‡åˆ°é—®é¢˜: {error_msg}"


def _filter_experiences_by_domain(
    domain: str, experiences: List[Dict], model: Model
) -> List[Dict]:
    """æ ¹æ®é¢†åŸŸç­›é€‰ç›¸å…³ç»éªŒ"""
    try:
        # ç”Ÿæˆé¢†åŸŸæŸ¥è¯¢å‘é‡
        domain_embedding = model.embed([domain])
        if not domain_embedding or len(domain_embedding.embeddings) == 0:
            # é™çº§ä¸ºå…³é”®è¯åŒ¹é…
            return _filter_by_keywords(domain, experiences)

        domain_vector = domain_embedding.embeddings
        relevant_experiences = []

        # è®¡ç®—æ¯ä¸ªç»éªŒä¸é¢†åŸŸçš„ç›¸å…³æ€§
        for exp in experiences:
            if "embedding" not in exp:
                continue

            similarity = _cosine_similarity(domain_vector, exp["embedding"])
            if similarity > 0.25:  # è¾ƒä½çš„é˜ˆå€¼ï¼ŒåŒ…å«æ›´å¤šç›¸å…³ç»éªŒ
                relevant_experiences.append(
                    {"experience": exp, "relevance": similarity}
                )

        # æŒ‰ç›¸å…³æ€§æ’åº
        relevant_experiences.sort(key=lambda x: x["relevance"], reverse=True)
        return [item["experience"] for item in relevant_experiences]

    except Exception as e:
        print_message(f"å‘é‡ç­›é€‰å¤±è´¥: {e}")
        return _filter_by_keywords(domain, experiences)


def _filter_by_keywords(domain: str, experiences: List[Dict]) -> List[Dict]:
    """å…³é”®è¯ç­›é€‰ç›¸å…³ç»éªŒ"""
    domain_keywords = domain.lower().split()
    relevant_experiences = []

    for exp in experiences:
        searchable_text = f"{exp.get('title', '')} {exp.get('summary', '')} {exp.get('category', '')} {' '.join(exp.get('tags', []))}".lower()

        # è®¡ç®—å…³é”®è¯åŒ¹é…åº¦
        match_count = sum(
            1 for keyword in domain_keywords if keyword in searchable_text
        )
        if match_count > 0:
            relevant_experiences.append(exp)

    print_message(f"å…³é”®è¯ç­›é€‰æ‰¾åˆ° {len(relevant_experiences)} æ¡ç»éªŒ")
    return relevant_experiences


def _build_abstraction_prompt(
    domain: str, pattern_type: str, experiences: List[Dict]
) -> str:
    """æ„å»ºç»éªŒæŠ½è±¡çš„æç¤ºè¯"""

    base_prompt = f"""
è¯·å¯¹ä»¥ä¸‹ '{domain}' é¢†åŸŸçš„ç»éªŒè¿›è¡Œæ·±åº¦æŠ½è±¡å’Œæ¨¡å¼æå–ã€‚

é¢†åŸŸ: {domain}
æŠ½è±¡ç±»å‹: {pattern_type}
ç»éªŒæ•°é‡: {len(experiences)}

ç›¸å…³ç»éªŒæ•°æ®:
"""

    # æ·»åŠ ç»éªŒå†…å®¹
    for i, exp in enumerate(experiences[:10]):  # é™åˆ¶æœ€å¤š10æ¡ç»éªŒ
        base_prompt += f"""
ç»éªŒ {i+1}:
- æ ‡é¢˜: {exp.get('title', 'æœªçŸ¥')}
- åˆ†ç±»: {exp.get('category', 'æœªçŸ¥')}
- é—®é¢˜: {exp.get('details', {}).get('problem', 'æ— ')}
- è§£å†³æ–¹æ¡ˆ: {exp.get('details', {}).get('solution', 'æ— ')}
- å…³é”®ç‚¹: {', '.join(exp.get('details', {}).get('key_points', []))}
- é™·é˜±: {', '.join(exp.get('details', {}).get('pitfalls', []))}
- é€‚ç”¨åœºæ™¯: {', '.join(exp.get('details', {}).get('applicable_scenarios', []))}
- éš¾åº¦: {exp.get('difficulty', 'æœªçŸ¥')}

"""

    # æ ¹æ®æŠ½è±¡ç±»å‹æ·»åŠ ç‰¹å®šæŒ‡å¯¼
    if pattern_type == "success_patterns":
        specific_prompt = """
è¯·é‡ç‚¹æå–æˆåŠŸæ¨¡å¼å’Œè§„å¾‹ï¼š
1. æˆåŠŸè§£å†³é—®é¢˜çš„å…±åŒæ–¹æ³•å’Œç­–ç•¥
2. é«˜æ•ˆè§£å†³æ–¹æ¡ˆçš„å…±åŒç‰¹å¾
3. æˆåŠŸæ¡ˆä¾‹ä¸­çš„å…³é”®å†³ç­–ç‚¹
4. å¯å¤ç”¨çš„æˆåŠŸæ¨¡å¼å’Œæ¡†æ¶
"""
    elif pattern_type == "failure_patterns":
        specific_prompt = """
è¯·é‡ç‚¹æå–å¤±è´¥æ¨¡å¼å’Œé™·é˜±ï¼š
1. å¸¸è§çš„é”™è¯¯å’Œé™·é˜±
2. å®¹æ˜“å¿½è§†çš„é£é™©ç‚¹
3. å¤±è´¥æ¡ˆä¾‹çš„å…±åŒç‰¹å¾
4. éœ€è¦é¿å…çš„è¡Œä¸ºæ¨¡å¼
"""
    elif pattern_type == "best_practices":
        specific_prompt = """
è¯·é‡ç‚¹æå–æœ€ä½³å®è·µï¼š
1. ç»è¿‡éªŒè¯çš„ä¼˜ç§€åšæ³•
2. æ•ˆç‡æœ€é«˜çš„å·¥ä½œæ–¹æ³•
3. é€šç”¨çš„è§£å†³æ¡†æ¶
4. æ ‡å‡†åŒ–çš„æ“ä½œæµç¨‹
"""
    else:  # "all"
        specific_prompt = """
è¯·è¿›è¡Œå…¨é¢çš„ç»éªŒæŠ½è±¡ï¼š
1. æˆåŠŸæ¨¡å¼å’Œè§„å¾‹
2. å¤±è´¥æ¨¡å¼å’Œé™·é˜±
3. æœ€ä½³å®è·µå’Œæ ‡å‡†æµç¨‹
4. é€šç”¨åŸåˆ™å’Œæ–¹æ³•è®º
"""

    final_prompt = (
        base_prompt
        + specific_prompt
        + """

è¯·æŒ‰ä»¥ä¸‹ç»“æ„è¾“å‡ºæŠ½è±¡ç»“æœï¼š

## æ ¸å¿ƒæ¨¡å¼è¯†åˆ«
[è¯†åˆ«å‡ºçš„ä¸»è¦æ¨¡å¼å’Œè§„å¾‹]

## é€šç”¨åŸåˆ™
[æå–çš„é€šç”¨åŸåˆ™å’Œæ–¹æ³•è®º]

## å†³ç­–æ¡†æ¶
[å½¢æˆçš„å†³ç­–å’Œåˆ¤æ–­æ¡†æ¶]

## å®è·µæŒ‡å—
[å…·ä½“çš„å®è·µæŒ‡å¯¼å»ºè®®]

## é£é™©é¢„è­¦
[éœ€è¦æ³¨æ„çš„é£é™©å’Œé™·é˜±]

## é€‚ç”¨æ¡ä»¶
[è¿™äº›æ¨¡å¼å’ŒåŸåˆ™çš„é€‚ç”¨èŒƒå›´å’Œæ¡ä»¶]

è¦æ±‚ï¼š
- æŠ½è±¡å±‚æ¬¡è¦é«˜äºå…·ä½“ç»éªŒï¼Œå½¢æˆå¯å¤ç”¨çš„æ¨¡å¼
- è¦æœ‰å®ç”¨ä»·å€¼ï¼Œèƒ½æŒ‡å¯¼æœªæ¥çš„ç±»ä¼¼é—®é¢˜
- ä¿æŒé€»è¾‘æ€§å’Œç»“æ„æ€§
- çªå‡ºå…±æ€§ï¼Œæ·¡åŒ–ä¸ªæ€§
"""
    )

    return final_prompt


def _save_abstract_pattern(
    domain: str, pattern_type: str, abstracted_content: str, source_count: int
):
    """å°†æŠ½è±¡æ¨¡å¼ä¿å­˜ä¸ºé«˜çº§ç»éªŒ"""
    try:
        # æ„å»ºæŠ½è±¡ç»éªŒè®°å½•
        abstract_experience = {
            "id": f"abstract_{int(time.time())}",
            "title": f"{domain}é¢†åŸŸ-{pattern_type}æŠ½è±¡æ¨¡å¼",
            "category": "æŠ½è±¡æ¨¡å¼",
            "tags": [domain, pattern_type, "æŠ½è±¡ç»éªŒ", "æ¨¡å¼"],
            "summary": f"ä»{source_count}æ¡ç»éªŒä¸­æŠ½è±¡å‡ºçš„{domain}é¢†åŸŸ{pattern_type}æ¨¡å¼",
            "details": {
                "problem": f"{domain}é¢†åŸŸçš„é€šç”¨é—®é¢˜æ¨¡å¼",
                "solution": f"æŠ½è±¡åŒ–çš„è§£å†³æ–¹æ¡ˆå’Œæ–¹æ³•è®º",
                "key_points": [
                    f"åŸºäº{source_count}æ¡ç»éªŒçš„æŠ½è±¡",
                    f"{pattern_type}æ¨¡å¼",
                    "é«˜çº§ç»éªŒ",
                ],
                "pitfalls": ["æŠ½è±¡å±‚æ¬¡è¾ƒé«˜ï¼Œéœ€è¦ç»“åˆå…·ä½“æƒ…å†µåº”ç”¨"],
                "applicable_scenarios": [f"{domain}ç›¸å…³çš„å„ç§åœºæ™¯"],
            },
            "difficulty": "ä¸­ç­‰",
            "usefulness": "5",
            "created_at": time.time(),
            "raw_content": abstracted_content,
            "source_experiences_count": source_count,
            "abstraction_type": pattern_type,
        }

        # è¯»å–ç°æœ‰ç»éªŒåº“
        experience_file = (
            "/Users/own/Workspace/MengLong/examples/app/terminal_try/experience.yaml"
        )
        experience_data = {"experiences": []}
        if os.path.exists(experience_file):
            with open(experience_file, "r", encoding="utf-8") as f:
                experience_data = yaml.safe_load(f) or {"experiences": []}

        # æ·»åŠ æŠ½è±¡ç»éªŒ
        experience_data["experiences"].append(abstract_experience)

        # ä¿å­˜å›æ–‡ä»¶
        with open(experience_file, "w", encoding="utf-8") as f:
            yaml.dump(
                experience_data,
                f,
                default_flow_style=False,
                allow_unicode=True,
                indent=2,
            )

        print_message(f"âœ… æŠ½è±¡æ¨¡å¼å·²ä¿å­˜ä¸ºé«˜çº§ç»éªŒ: {abstract_experience['id']}")

    except Exception as e:
        print_message(f"âš ï¸ æŠ½è±¡æ¨¡å¼ä¿å­˜å¤±è´¥: {e}")


@tool
def debug_experience_db() -> str:
    """
    è°ƒè¯•ç»éªŒåº“å†…å®¹ï¼Œæ£€æŸ¥æ ¼å¼å’Œæ•°æ®

    è¿”å›:
        str: ç»éªŒåº“çš„è°ƒè¯•ä¿¡æ¯
    """
    print_message("ğŸ” å¼€å§‹è°ƒè¯•ç»éªŒåº“...")

    try:
        # ç»éªŒåº“æ–‡ä»¶è·¯å¾„
        experience_file = (
            "/Users/own/Workspace/MengLong/examples/app/terminal_try/experience.yaml"
        )

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(experience_file):
            return "âŒ ç»éªŒåº“æ–‡ä»¶ä¸å­˜åœ¨"

        # è¯»å–ç»éªŒåº“
        with open(experience_file, "r", encoding="utf-8") as f:
            experience_data = yaml.safe_load(f) or {}

        experiences = experience_data.get("experiences", [])

        debug_info = f"""
ğŸ“Š ç»éªŒåº“è°ƒè¯•æŠ¥å‘Š
===================

ğŸ“ æ–‡ä»¶è·¯å¾„: {experience_file}
ğŸ“ˆ ç»éªŒæ€»æ•°: {len(experiences)}

"""

        if not experiences:
            debug_info += "âš ï¸ ç»éªŒåº“ä¸ºç©º\n"
            return debug_info

        # æ£€æŸ¥æ¯ä¸ªç»éªŒçš„æ ¼å¼
        for i, exp in enumerate(experiences):
            debug_info += f"""
ç»éªŒ {i+1}:
  ID: {exp.get('id', 'Missing')}
  æ ‡é¢˜: {exp.get('title', 'Missing')}
  åˆ†ç±»: {exp.get('category', 'Missing')}
  æ ‡ç­¾: {exp.get('tags', [])}
  å‘é‡: {'âœ… å­˜åœ¨' if 'embedding' in exp and exp['embedding'] else 'âŒ ç¼ºå¤±'}
"""
            if "embedding" in exp and exp["embedding"]:
                debug_info += f"  å‘é‡ç»´åº¦: {len(exp['embedding'])}\n"

            # æ£€æŸ¥å¿…è¦å­—æ®µ
            required_fields = ["title", "summary", "details"]
            missing_fields = [field for field in required_fields if field not in exp]
            if missing_fields:
                debug_info += f"  âš ï¸ ç¼ºå¤±å­—æ®µ: {missing_fields}\n"

        # ç»Ÿè®¡ä¿¡æ¯
        has_embedding = sum(
            1 for exp in experiences if "embedding" in exp and exp["embedding"]
        )
        debug_info += f"""
ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:
  æœ‰å‘é‡çš„ç»éªŒ: {has_embedding}/{len(experiences)}
  å‘é‡åŒ–ç‡: {has_embedding/len(experiences)*100:.1f}%
"""

        return debug_info

    except Exception as e:
        return f"âŒ è°ƒè¯•å¤±è´¥: {str(e)}"


@tool
def test_embedding_functionality() -> str:
    """
    æµ‹è¯• embedding åŠŸèƒ½æ˜¯å¦æ­£å¸¸å·¥ä½œ

    è¿”å›:
        str: æµ‹è¯•ç»“æœ
    """
    print_message("ğŸ§ª å¼€å§‹æµ‹è¯• embedding åŠŸèƒ½...")

    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = Model()

        # æµ‹è¯•æ–‡æœ¬
        test_texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬", "ç¼–ç¨‹è°ƒè¯•æŠ€å·§", "é¡¹ç›®ç®¡ç†ç»éªŒ"]

        test_results = []

        for text in test_texts:
            try:
                print_message(f"æµ‹è¯•æ–‡æœ¬: {text}")
                embed_response = model.embed([text])

                if embed_response and hasattr(embed_response, "embeddings"):
                    if len(embed_response.embeddings) > 0:
                        vector = embed_response.embeddings[0]
                        test_results.append(f"âœ… '{text}' - å‘é‡ç»´åº¦: {len(vector)}")
                        print_message(f"å‘é‡å‰5ä¸ªå€¼: {vector[:5]}")
                    else:
                        test_results.append(f"âŒ '{text}' - embeddings åˆ—è¡¨ä¸ºç©º")
                else:
                    test_results.append(f"âŒ '{text}' - æ²¡æœ‰ embeddings å±æ€§")

            except Exception as e:
                test_results.append(f"âŒ '{text}' - é”™è¯¯: {str(e)}")

        # æµ‹è¯•ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
        if len(test_results) >= 2 and all(
            "âœ…" in result for result in test_results[:2]
        ):
            try:
                vec1 = model.embed([test_texts[0]]).embeddings[0]
                vec2 = model.embed([test_texts[1]]).embeddings[0]
                similarity = _cosine_similarity(vec1, vec2)
                test_results.append(f"âœ… ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—: {similarity:.4f}")
            except Exception as e:
                test_results.append(f"âŒ ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")

        result = "ğŸ§ª Embedding åŠŸèƒ½æµ‹è¯•ç»“æœ:\n" + "\n".join(test_results)
        return result

    except Exception as e:
        return f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}"


def _find_similar_experience_groups(
    experiences: List[Dict], threshold: float, model: Model
) -> List[List[Dict]]:
    """æ‰¾åˆ°ç›¸ä¼¼çš„ç»éªŒç»„"""
    try:
        # ä¸ºæ²¡æœ‰å‘é‡çš„ç»éªŒç”Ÿæˆå‘é‡
        for exp in experiences:
            if "embedding" not in exp or not exp["embedding"]:
                searchable_text = f"{exp.get('title', '')} {exp.get('summary', '')} {' '.join(exp.get('tags', []))}"
                try:
                    embed_response = model.embed([searchable_text])
                    if (
                        embed_response
                        and hasattr(embed_response, "embeddings")
                        and len(embed_response.embeddings) > 0
                    ):
                        exp["embedding"] = embed_response.embeddings[0]
                        print_message(
                            f"ä¸ºç»éªŒ '{exp.get('title', 'Unknown')}' ç”Ÿæˆå‘é‡"
                        )
                except Exception as e:
                    print_message(f"å‘é‡ç”Ÿæˆå¤±è´¥: {exp.get('title', 'Unknown')} - {e}")

        # è®¡ç®—æ‰€æœ‰ç»éªŒä¹‹é—´çš„ç›¸ä¼¼åº¦
        similar_groups = []
        processed = set()

        for i, exp1 in enumerate(experiences):
            if exp1.get("id") in processed or "embedding" not in exp1:
                continue

            current_group = [exp1]
            processed.add(exp1.get("id"))

            for j, exp2 in enumerate(experiences[i + 1 :], i + 1):
                if exp2.get("id") in processed or "embedding" not in exp2:
                    continue

                # è®¡ç®—ç›¸ä¼¼åº¦
                similarity = _cosine_similarity(exp1["embedding"], exp2["embedding"])

                if similarity > threshold:
                    current_group.append(exp2)
                    processed.add(exp2.get("id"))
                    print_message(
                        f"å‘ç°ç›¸ä¼¼ç»éªŒ: '{exp1.get('title', 'Unknown')}' ä¸ '{exp2.get('title', 'Unknown')}' ç›¸ä¼¼åº¦: {similarity:.3f}"
                    )

            if len(current_group) > 1:
                similar_groups.append(current_group)

        return similar_groups

    except Exception as e:
        print_message(f"æŸ¥æ‰¾ç›¸ä¼¼ç»éªŒç»„å¤±è´¥: {e}")
        return []


def _merge_similar_experiences(experiences: List[Dict], model: Model) -> Dict:
    """åˆå¹¶ç›¸ä¼¼çš„ç»éªŒä¸ºä¸€æ¡æ›´å¥½çš„ç»éªŒ"""
    try:
        # æ„å»ºåˆå¹¶æç¤ºè¯
        merge_prompt = f"""
è¯·å°†ä»¥ä¸‹ {len(experiences)} æ¡ç›¸ä¼¼çš„ç»éªŒåˆå¹¶ä¸ºä¸€æ¡æ›´å®Œæ•´ã€æ›´ä¼˜è´¨çš„ç»éªŒè®°å½•ã€‚

ç›¸ä¼¼ç»éªŒåˆ—è¡¨:
"""

        for i, exp in enumerate(experiences):
            merge_prompt += f"""
ç»éªŒ {i+1}:
- æ ‡é¢˜: {exp.get('title', 'æœªçŸ¥')}
- åˆ†ç±»: {exp.get('category', 'æœªçŸ¥')}
- æ ‡ç­¾: {exp.get('tags', [])}
- æ‘˜è¦: {exp.get('summary', 'æ— æ‘˜è¦')}
- é—®é¢˜: {exp.get('details', {}).get('problem', 'æ— ')}
- è§£å†³æ–¹æ¡ˆ: {exp.get('details', {}).get('solution', 'æ— ')}
- å…³é”®ç‚¹: {exp.get('details', {}).get('key_points', [])}
- é™·é˜±: {exp.get('details', {}).get('pitfalls', [])}
- é€‚ç”¨åœºæ™¯: {exp.get('details', {}).get('applicable_scenarios', [])}
- éš¾åº¦: {exp.get('difficulty', 'æœªçŸ¥')}
- å®ç”¨æ€§: {exp.get('usefulness', 'æœªçŸ¥')}

"""

        merge_prompt += """
è¯·æŒ‰ä»¥ä¸‹è¦æ±‚åˆå¹¶:
1. æå–æœ€ç²¾å‡†çš„æ ‡é¢˜ï¼Œæ¶µç›–æ‰€æœ‰ç»éªŒçš„æ ¸å¿ƒä¸»é¢˜
2. é€‰æ‹©æœ€åˆé€‚çš„åˆ†ç±»
3. åˆå¹¶æ‰€æœ‰ç›¸å…³æ ‡ç­¾ï¼Œå»é™¤é‡å¤
4. ç»¼åˆæ‰€æœ‰æ‘˜è¦ï¼Œå½¢æˆæ›´å®Œæ•´çš„æè¿°
5. æ•´åˆé—®é¢˜æè¿°ï¼Œè¦†ç›–æ‰€æœ‰åœºæ™¯
6. åˆå¹¶è§£å†³æ–¹æ¡ˆï¼Œä¿ç•™æœ€æœ‰æ•ˆçš„æ–¹æ³•
7. æ±‡æ€»æ‰€æœ‰å…³é”®ç‚¹ï¼Œå»é‡å¹¶æ’åº
8. æ•´åˆæ‰€æœ‰é™·é˜±å’Œæ³¨æ„äº‹é¡¹
9. æ‰©å±•é€‚ç”¨åœºæ™¯è¦†ç›–èŒƒå›´
10. è¯„ä¼°ç»¼åˆéš¾åº¦å’Œå®ç”¨æ€§

è¿”å›JSONæ ¼å¼:
{
  "title": "åˆå¹¶åçš„æ ‡é¢˜",
  "category": "æœ€åˆé€‚çš„åˆ†ç±»",
  "tags": ["åˆå¹¶åçš„æ ‡ç­¾åˆ—è¡¨"],
  "summary": "ç»¼åˆæ‘˜è¦",
  "details": {
    "problem": "æ•´åˆåçš„é—®é¢˜æè¿°",
    "solution": "ç»¼åˆè§£å†³æ–¹æ¡ˆ",
    "key_points": ["æ•´åˆåçš„å…³é”®ç‚¹åˆ—è¡¨"],
    "pitfalls": ["åˆå¹¶åçš„é™·é˜±åˆ—è¡¨"],
    "applicable_scenarios": ["æ‰©å±•åçš„é€‚ç”¨åœºæ™¯"]
  },
  "difficulty": "è¯„ä¼°åçš„éš¾åº¦",
  "usefulness": "è¯„ä¼°åçš„å®ç”¨æ€§"
}

åªè¿”å›JSONï¼Œä¸è¦å…¶ä»–è§£é‡Šã€‚
"""

        # ä½¿ç”¨LLMåˆå¹¶ç»éªŒ
        response = model.chat([user(content=merge_prompt)])
        merge_text = response.message.content.text

        # è§£æJSON
        if merge_text.startswith("```json"):
            merge_text = merge_text[7:]
        if merge_text.endswith("```"):
            merge_text = merge_text[:-3]

        merged_experience = json.loads(merge_text)

        # æ·»åŠ å…ƒæ•°æ®
        merged_experience["id"] = f"merged_{int(time.time())}"
        merged_experience["created_at"] = time.time()
        merged_experience["merged_from"] = [exp.get("id") for exp in experiences]
        merged_experience["merge_count"] = len(experiences)

        # ç”Ÿæˆåˆå¹¶åçš„åŸå§‹å†…å®¹
        raw_content = f"åˆå¹¶è‡ª {len(experiences)} æ¡ç»éªŒ:\n"
        for exp in experiences:
            raw_content += f"- {exp.get('title', 'Unknown')}\n"
        raw_content += f"\nç»¼åˆå†…å®¹: {merged_experience.get('summary', '')}"
        merged_experience["raw_content"] = raw_content

        # ç”Ÿæˆæ–°çš„å‘é‡
        searchable_text = f"{merged_experience.get('title', '')} {merged_experience.get('summary', '')} {' '.join(merged_experience.get('tags', []))}"
        try:
            embed_response = model.embed([searchable_text])
            if (
                embed_response
                and hasattr(embed_response, "embeddings")
                and len(embed_response.embeddings) > 0
            ):
                merged_experience["embedding"] = embed_response.embeddings[0]
        except Exception as e:
            print_message(f"åˆå¹¶ç»éªŒå‘é‡åŒ–å¤±è´¥: {e}")

        print_message(f"æˆåŠŸåˆå¹¶ç»éªŒ: {merged_experience.get('title', 'Unknown')}")
        return merged_experience

    except Exception as e:
        print_message(f"åˆå¹¶ç»éªŒå¤±è´¥: {e}")
        return None


@tool
def merge_similar_experiences(
    similarity_threshold: float = 0.7, domain_filter: str = ""
) -> str:
    """
    æ•´ç†ç»éªŒåº“ï¼Œåˆå¹¶ç›¸ä¼¼çš„ç»éªŒä¸ºæ›´å¥½çš„å•æ¡ç»éªŒ

    å‚æ•°:
        similarity_threshold (float): ç›¸ä¼¼åº¦é˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼çš„ç»éªŒå°†è¢«åˆå¹¶ (é»˜è®¤ 0.7)
        domain_filter (str): å¯é€‰çš„é¢†åŸŸè¿‡æ»¤å™¨ï¼Œåªæ•´ç†ç‰¹å®šé¢†åŸŸçš„ç»éªŒ

    è¿”å›:
        str: æ•´ç†ç»“æœæŠ¥å‘Š
    """
    print_message(f"ğŸ”„ å¼€å§‹æ•´ç†ç»éªŒåº“ï¼Œç›¸ä¼¼åº¦é˜ˆå€¼: {similarity_threshold}")

    try:
        # åˆå§‹åŒ–æ¨¡å‹
        model = Model()

        # ç»éªŒåº“æ–‡ä»¶è·¯å¾„
        experience_file = (
            "/Users/own/Workspace/MengLong/examples/app/terminal_try/experience.yaml"
        )

        # æ£€æŸ¥ç»éªŒåº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(experience_file):
            return "ç»éªŒåº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œæ— æ³•è¿›è¡Œæ•´ç†ã€‚"

        # è¯»å–ç»éªŒåº“
        try:
            with open(experience_file, "r", encoding="utf-8") as f:
                experience_data = yaml.safe_load(f) or {}
        except Exception as e:
            print_message(f"è¯»å–ç»éªŒåº“å¤±è´¥: {e}")
            return "æ— æ³•è¯»å–ç»éªŒåº“æ–‡ä»¶ã€‚"

        experiences = experience_data.get("experiences", [])
        if len(experiences) < 2:
            return "ç»éªŒåº“ä¸­ç»éªŒæ•°é‡ä¸è¶³ï¼Œæ— éœ€æ•´ç†ã€‚"

        original_count = len(experiences)
        print_message(f"åŸå§‹ç»éªŒæ•°é‡: {original_count}")

        # è¿‡æ»¤é¢†åŸŸï¼ˆå¦‚æœæŒ‡å®šï¼‰
        if domain_filter:
            experiences = _filter_by_keywords(domain_filter, experiences)
            if len(experiences) < 2:
                return f"åœ¨ '{domain_filter}' é¢†åŸŸä¸­æ‰¾åˆ°çš„ç»éªŒæ•°é‡ä¸è¶³ï¼Œæ— éœ€æ•´ç†ã€‚"
            print_message(f"é¢†åŸŸè¿‡æ»¤åç»éªŒæ•°é‡: {len(experiences)}")

        # æ‰¾åˆ°ç›¸ä¼¼çš„ç»éªŒç»„
        similar_groups = _find_similar_experience_groups(
            experiences, similarity_threshold, model
        )

        if not similar_groups:
            return "æœªæ‰¾åˆ°éœ€è¦åˆå¹¶çš„ç›¸ä¼¼ç»éªŒã€‚è¯·å°è¯•é™ä½ç›¸ä¼¼åº¦é˜ˆå€¼ã€‚"

        print_message(f"æ‰¾åˆ° {len(similar_groups)} ä¸ªç›¸ä¼¼ç»éªŒç»„")

        # åˆå¹¶æ¯ç»„ç›¸ä¼¼ç»éªŒ
        merged_count = 0
        new_experiences = []
        processed_ids = set()

        for i, group in enumerate(similar_groups):
            if len(group) >= 2:  # åªå¤„ç†æœ‰å¤šä¸ªç»éªŒçš„ç»„
                print_message(f"æ­£åœ¨åˆå¹¶ç¬¬ {i+1} ç»„ç»éªŒ...")
                merged_exp = _merge_similar_experiences(group, model)
                if merged_exp:
                    new_experiences.append(merged_exp)
                    processed_ids.update(
                        exp.get("id") for exp in group if exp.get("id")
                    )
                    merged_count += len(group)
                    titles = [exp.get("title", "Unknown") for exp in group]
                    print_message(f"âœ… åˆå¹¶äº† {len(group)} æ¡ç›¸ä¼¼ç»éªŒ: {titles}")
                else:
                    print_message(f"âŒ ç¬¬ {i+1} ç»„ç»éªŒåˆå¹¶å¤±è´¥")

        # ä¿ç•™æœªè¢«åˆå¹¶çš„ç»éªŒ
        if domain_filter:
            # å¦‚æœæœ‰é¢†åŸŸè¿‡æ»¤ï¼Œéœ€è¦é‡æ–°è¯»å–å®Œæ•´çš„ç»éªŒåº“
            with open(experience_file, "r", encoding="utf-8") as f:
                full_experience_data = yaml.safe_load(f) or {}
            all_experiences = full_experience_data.get("experiences", [])
            remaining_experiences = [
                exp for exp in all_experiences if exp.get("id") not in processed_ids
            ]
        else:
            remaining_experiences = [
                exp for exp in experiences if exp.get("id") not in processed_ids
            ]

        # æ›´æ–°ç»éªŒåº“
        final_experiences = remaining_experiences + new_experiences
        experience_data["experiences"] = final_experiences

        # ä¿å­˜æ›´æ–°åçš„ç»éªŒåº“
        with open(experience_file, "w", encoding="utf-8") as f:
            yaml.dump(
                experience_data,
                f,
                default_flow_style=False,
                allow_unicode=True,
                indent=2,
            )

        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        final_count = len(final_experiences)
        reduced_count = original_count - final_count
        compression_ratio = (
            (reduced_count / original_count * 100) if original_count > 0 else 0
        )

        result = f"""
âœ… ç»éªŒåº“æ•´ç†å®Œæˆ

ğŸ“Š æ•´ç†ç»Ÿè®¡:
  åŸå§‹ç»éªŒæ•°: {original_count}
  ç›¸ä¼¼ç»éªŒç»„æ•°: {len(similar_groups)}
  è¢«åˆå¹¶çš„ç»éªŒæ•°: {merged_count}
  æ–°ç”Ÿæˆçš„ç»éªŒæ•°: {len(new_experiences)}
  ä¿ç•™çš„ç»éªŒæ•°: {len(remaining_experiences)}
  æœ€ç»ˆç»éªŒæ•°: {final_count}
  
ğŸ“ˆ ä¼˜åŒ–æ•ˆæœ:
  å‡å°‘ç»éªŒæ•°: {reduced_count}
  å‹ç¼©æ¯”: {compression_ratio:.1f}%
  
ğŸ’¡ å»ºè®®:
  {'ç»éªŒåº“å·²å¾—åˆ°æœ‰æ•ˆæ•´ç†ï¼Œé‡å¤ç»éªŒå·²åˆå¹¶ã€‚' if reduced_count > 0 else 'å½“å‰ç›¸ä¼¼åº¦é˜ˆå€¼ä¸‹æœªæ‰¾åˆ°å¯åˆå¹¶çš„ç»éªŒï¼Œå¯å°è¯•é™ä½é˜ˆå€¼ã€‚'}
"""

        return result

    except Exception as e:
        error_msg = f"ç»éªŒåº“æ•´ç†å¤±è´¥: {str(e)}"
        print_message(f"âŒ {error_msg}", MessageType.ERROR)
        import traceback

        print_message(f"è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return f"ç»éªŒåº“æ•´ç†é‡åˆ°é—®é¢˜: {error_msg}"


@tool
def test_merge_workflow() -> str:
    """
    æµ‹è¯•ç»éªŒåˆå¹¶å·¥ä½œæµç¨‹

    è¿”å›:
        str: æµ‹è¯•ç»“æœ
    """
    print_message("ğŸ§ª å¼€å§‹æµ‹è¯•ç»éªŒåˆå¹¶å·¥ä½œæµç¨‹...")

    try:
        # 1. æ£€æŸ¥ç»éªŒåº“
        debug_result = debug_experience_db()
        print_message("âœ… ç»éªŒåº“æ£€æŸ¥å®Œæˆ")

        # 2. æµ‹è¯•embeddingåŠŸèƒ½
        embed_result = test_embedding_functionality()
        print_message("âœ… EmbeddingåŠŸèƒ½æµ‹è¯•å®Œæˆ")

        # 3. å°è¯•åˆå¹¶ï¼ˆä½¿ç”¨è¾ƒä½çš„é˜ˆå€¼è¿›è¡Œæµ‹è¯•ï¼‰
        merge_result = merge_similar_experiences(similarity_threshold=0.3)
        print_message("âœ… åˆå¹¶æµ‹è¯•å®Œæˆ")

        return f"""
ğŸ§ª ç»éªŒåˆå¹¶å·¥ä½œæµç¨‹æµ‹è¯•ç»“æœ

ğŸ“‹ ç»éªŒåº“çŠ¶æ€:
{debug_result}

ğŸ”§ Embeddingæµ‹è¯•:
{embed_result}

ğŸ”„ åˆå¹¶æµ‹è¯•:
{merge_result}
"""

    except Exception as e:
        return f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}"


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    print(test_merge_workflow())
